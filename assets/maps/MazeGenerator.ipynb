{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération du Labyrinthe\n",
    "\n",
    "> L'objectif de ce notebook est d'entrainer un modèle à générer des labyrinthes complexes et infinis en utilisant les cartes d'exemple disponibles dans ce même répertoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération du dataset\n",
    "\n",
    "Dans un premier temps, nous cherchons à obtenir un dataset de la forme suivante :\n",
    "$$\n",
    "\\text{Voisinage} \\rightarrow \\text{Case}\n",
    "$$\n",
    "Pour ce faire, nous allons reprendre l'idée des filtres de convolution et balayer le labyrinthe avec un filtre de taille $n\\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "-> **training_map** : *2 layers*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------- CHARGEMENT DES DONNEES -------------------------- #\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "data = {}\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for filename in sorted(files):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            last_character = filename[-5]\n",
    "            simple_filename = filename.split(\".\")[0]\n",
    "            if simple_filename not in data:\n",
    "                data[simple_filename] = {}\n",
    "            if last_character not in data[simple_filename]:\n",
    "                try:\n",
    "                    csv = pd.read_csv(filename).to_numpy()\n",
    "                except:\n",
    "                    continue\n",
    "                if csv.shape[0] * csv.shape[1] != 0:\n",
    "                    data[simple_filename][last_character] = csv\n",
    "\n",
    "for data_name in data:\n",
    "    display(Markdown(f\"-> **{data_name}** : *{len(data[data_name])} layers*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "-> **training_map** : *1 : 360 samples*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-> **training_map** : *2 : 360 samples*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------- CREATON DES DATASETS --------------------------- #\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_datasets(data_list: list, neighbourhood: int = 2):\n",
    "    window_size = neighbourhood * 2 + 1\n",
    "\n",
    "    datasets = {}\n",
    "    for key, data in data_list.items():\n",
    "        # Add padding of size window_size // 2 with -1\n",
    "        data = data.astype(str)\n",
    "        data = np.pad(data, window_size, constant_values=\"border\")\n",
    "        dataset = pd.DataFrame(\n",
    "            [\n",
    "                data[i : i + window_size, j : j + window_size].flatten()\n",
    "                for i in range(0, data.shape[0] - window_size + 1)\n",
    "                for j in range(0, data.shape[1] - window_size + 1)\n",
    "            ]\n",
    "        )\n",
    "        dataset[\"label\"] = dataset[dataset.columns[(window_size**2 - 1) // 2]]\n",
    "        dataset.drop(dataset.columns[(window_size**2 - 1) // 2], axis=1, inplace=True)\n",
    "        dataset = dataset[dataset[\"label\"] != \"border\"]\n",
    "        dataset = dataset.replace(\"border\", -1)\n",
    "        dataset = dataset.astype(int)\n",
    "\n",
    "        datasets[key] = dataset\n",
    "\n",
    "    result_datasets = {}\n",
    "    for key in datasets:\n",
    "        result_datasets[key] = (\n",
    "            pd.concat(\n",
    "                [dataset.drop(columns=[\"label\"]) for dataset in datasets.values()]\n",
    "                + [datasets[key][\"label\"]],\n",
    "                axis=1,\n",
    "            )\n",
    "            .fillna(-1)\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "    return result_datasets\n",
    "\n",
    "\n",
    "datasets = {data_name: create_datasets(data[data_name]) for data_name in data}\n",
    "for dataset in datasets:\n",
    "    for layer in datasets[dataset]:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"-> **{dataset}** : *{layer} : {len(datasets[dataset][layer])} samples*\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ TRAINING MODELS ----------------------------- #\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers = {\n",
    "    dataset: {\n",
    "        layer: {\n",
    "            \"model\": RandomForestClassifier(verbose=True, n_jobs=-1).fit(\n",
    "                datasets[dataset][layer].drop(\"label\", axis=1),\n",
    "                datasets[dataset][layer][\"label\"],\n",
    "            ),\n",
    "            \"window_size\": int(\n",
    "                (np.sqrt(datasets[dataset][layer].shape[1] - 1) // 2 + 1)\n",
    "            ),\n",
    "            # List of unique value in the dataframe\n",
    "            \"assets\": dict(\n",
    "                zip(\n",
    "                    *np.unique(\n",
    "                        datasets[dataset][layer].to_numpy().flatten(),\n",
    "                        return_counts=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "        for layer in datasets[dataset]\n",
    "    }\n",
    "    for dataset in datasets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ GENERATING MAZE ----------------------------- #\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_maze(classifiers: dict, size: tuple, nbr_iter=5, temperature=0.1):\n",
    "    mazes = {}\n",
    "\n",
    "    # Unpack Arguments\n",
    "    size_x, size_y = size\n",
    "    window_size = classifiers[list(classifiers.keys())[0]][\"window_size\"]\n",
    "    assets_list = {key: classifier[\"assets\"] for key, classifier in classifiers.items()}\n",
    "    models = {key: classifier[\"model\"] for key, classifier in classifiers.items()}\n",
    "\n",
    "    # Generate Maze\n",
    "    ## Generate a matrix full of random values from the assets\n",
    "    mazes = {\n",
    "        key: np.random.choice(\n",
    "            list(assets.keys()),\n",
    "            (size_x, size_y),\n",
    "            p=list(assets.values()) / np.sum(list(assets.values())),\n",
    "        )\n",
    "        for key, assets in assets_list.items()\n",
    "    }\n",
    "\n",
    "    # Apply the classifier on the maze\n",
    "    for _ in range(nbr_iter):\n",
    "        # Generate dataset from the maze\n",
    "        datasets = create_datasets(mazes, window_size // 2)\n",
    "\n",
    "        for key, model in models.items():\n",
    "            # Apply the classifier on the dataset\n",
    "            logits = model.predict_proba(datasets[key].drop(\"label\", axis=1))\n",
    "            logits = logits ** (1 / temperature)\n",
    "            predictions = np.array(\n",
    "                [\n",
    "                    np.random.choice(model.classes_, p=logit / logit.sum())\n",
    "                    for logit in logits\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Recreate the maze from the predictions\n",
    "            mazes[key] = np.array(predictions).reshape(size_x, size_y)\n",
    "\n",
    "    return mazes\n",
    "\n",
    "\n",
    "mazes = generate_maze(classifiers[\"training_map\"], (25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAAP3aGbAAAEVElEQVR4nO3cP2sUQRgH4PgnnVY2iVfdFWmvSBcLCwtri4BfQPAzBPwAfo8UQgoLEU7QwuLSiEUKm1OSIoQ7O8FeLBaW43LmbnDnZmb3earNZAKb5sfsOzPv1hYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsGm3Ur8AlGQwGFQPe7s7k+msHt/b3akeJtPZwq+izj8/P//P/6gsAgsCDAaDOjtWGvZ7J6dfo85/fXyy5uR2uJ36BaAk66fJZuZ3jcACiiGwIMD1YlPa+V0jsCCAT8K07qR+ASjJn+3tB/fvrTn556/fk+ks6vwfl5drTm4HKyyIZdjvZTW/BQQWxHJ2cZXV/Ba4m/oFoCRBNaYNrLBG46C/KJ4VFlAMRXcIoOielhUWUAyBBbHYJWycwAKKYZcQAhwe7FcP1ZGCYb93dnE1v9JZ+DH2/K7RXgYC6IeVlsCCAPphpaWGBQFcfk5LYAHFEFgQQD+stAQWBPBJmJarORDA1Zy0rLAgFifdGyewIBb9sBrnpDsE0A8rLSssoBiK7hBA0T0tKyygGAILYrFL2DiBBRTDLiEE0A8rLe1lIKLY/bP0wwIaE7t/ln5YQGNcfm6WwAKKIbAgIv2wmiWwICKfhM1yNQciin2Vx9UcIA0n3VcSWJAL/bBWElgQUWj/rKj9tlpAYAHFUHSHiBTdm2WFBRRDYEEu7BKuJLCAYuiHBRHph9Us7WUgI6H9s/TDApIJ7Z+lHxaQjMvPNxNYkKPJdPb+47uFbjNLBztFYEFGqjCaj6T6eelg1wgsyEj1SXh4sP/w1dHjN2+/f/tS7xsuHewaRXfISF10r88uzLdkuD44Gp9u9gUTcw4LcrS0dczC4LDfG4039UJ58EkIpdIPC0hp/oDoyl3CDh55F1iQHbuE/6LoDhmpiu7Dfm/85Gk18ujTh/peYTX4+fmzoxcvq8GuFd0FFmTELuHN7BJCjuwSLqWGBRTDCgsyoh8WAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEA7/AWoQ+V/IZYO1gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------- PLOT MAP --------------------------------- #\n",
    "import os\n",
    "import pygame\n",
    "import itertools\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "def plot_map(mazes: dict):\n",
    "    width, height = mazes[list(mazes.keys())[0]].shape\n",
    "\n",
    "    assets = {}\n",
    "    for root, dirs, files in os.walk(os.path.join(\"..\", \"frames\")):\n",
    "        # Remove files starting with a capital letter\n",
    "        files = [f for f in files if not f[0].isupper()]\n",
    "        for i, filename in enumerate(sorted(files)):\n",
    "            if i in list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    [maze.flatten().tolist() for maze in mazes.values()]\n",
    "                )\n",
    "            ):\n",
    "                assets[i] = pygame.image.load(os.path.join(root, filename))\n",
    "    # Create a black image for -1 (16x16) with alpha channel\n",
    "    assets[-1] = pygame.Surface((16, 16), pygame.SRCALPHA)\n",
    "\n",
    "    # Create window\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((width * 16, height * 16))\n",
    "    pygame.display.set_caption(\"Maze\")\n",
    "    screen.fill((0, 0, 0))\n",
    "\n",
    "    # Draw the mazes\n",
    "    for maze in mazes.values():\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                screen.blit(assets[maze[i, j]], (i * 16, j * 16), special_flags=0)\n",
    "\n",
    "    pygame.display.flip()\n",
    "    pygame.image.save(screen, \"maze.png\")\n",
    "    pygame.quit()\n",
    "\n",
    "    # Display the maze\n",
    "    display(Image(filename=\"maze.png\"))\n",
    "\n",
    "\n",
    "plot_map(mazes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
