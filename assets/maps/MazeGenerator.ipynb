{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération du Labyrinthe\n",
    "\n",
    "> L'objectif de ce notebook est d'entrainer un modèle à générer des labyrinthes complexes et infinis en utilisant les cartes d'exemple disponibles dans ce même répertoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération du dataset\n",
    "\n",
    "Dans un premier temps, nous cherchons à obtenir un dataset de la forme suivante :\n",
    "$$\n",
    "\\text{Voisinage} \\rightarrow \\text{Case}\n",
    "$$\n",
    "Pour ce faire, nous allons reprendre l'idée des filtres de convolution et balayer le labyrinthe avec un filtre de taille $n\\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "-> **training_map** : *1 layers*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------- CHARGEMENT DES DONNEES -------------------------- #\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "data = {}\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            last_character = filename[-5]\n",
    "            simple_filename = filename.split(\".\")[0]\n",
    "            if simple_filename not in data:\n",
    "                data[simple_filename] = {}\n",
    "            if last_character not in data[simple_filename]:\n",
    "                csv = pd.read_csv(filename).to_numpy()\n",
    "                if csv.shape[0] * csv.shape[1] != 0:\n",
    "                    data[simple_filename][last_character] = csv\n",
    "\n",
    "for data_name in data:\n",
    "    display(Markdown(f\"-> **{data_name}** : *{len(data[data_name])} layers*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12709/3593577747.py:23: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataset_copy_masked.loc[:, mask] = dataset_copy_masked.loc[:, mask].applymap(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "-> **training_map** : *1 : 18 samples*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------- CREATON DES DATASETS --------------------------- #\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_dataset(data, window_size=2):\n",
    "    if data.shape[0] < window_size or data.shape[1] < window_size:\n",
    "        raise ValueError(\"Window size is too big for this data\")\n",
    "    dataset = pd.DataFrame(\n",
    "        [\n",
    "            data[i : i + window_size, j : j + window_size].flatten()\n",
    "            for i in range(0, data.shape[0] - window_size + 1)\n",
    "            for j in range(0, data.shape[1] - window_size + 1)\n",
    "        ]\n",
    "    )\n",
    "    dataset[\"label\"] = dataset[dataset.columns[(window_size**2 - 1) // 2]]\n",
    "    dataset.drop(dataset.columns[(window_size**2 - 1) // 2], axis=1, inplace=True)\n",
    "\n",
    "    # Data augmentation with dropouts\n",
    "    dataset_missing_value = dataset.drop(columns=\"label\").max(axis=1).max() + 1\n",
    "    dataset_copy = dataset.sample(frac=1)\n",
    "    mask = dataset_copy.columns != \"label\"\n",
    "    dataset_copy_masked = dataset_copy.copy()\n",
    "    dataset_copy_masked.loc[:, mask] = dataset_copy_masked.loc[:, mask].applymap(\n",
    "        lambda x: dataset_missing_value if np.random.rand() < 0.5 else x\n",
    "    )\n",
    "\n",
    "    dataset = pd.concat(\n",
    "        [\n",
    "            dataset,\n",
    "            dataset_copy_masked,\n",
    "        ]\n",
    "    ).drop_duplicates()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    data_name: {\n",
    "        last_character: create_dataset(data[data_name][last_character])\n",
    "        for last_character in data[data_name]\n",
    "    }\n",
    "    for data_name in data\n",
    "}\n",
    "for dataset in datasets:\n",
    "    for layer in datasets[dataset]:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"-> **{dataset}** : *{layer} : {len(datasets[dataset][layer])} samples*\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ TRAINING MODELS ----------------------------- #\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers = {\n",
    "    dataset: {\n",
    "        layer: {\n",
    "            \"model\": RandomForestClassifier(verbose=True, n_jobs=-1).fit(\n",
    "                datasets[dataset][layer].drop(\"label\", axis=1),\n",
    "                datasets[dataset][layer][\"label\"],\n",
    "            ),\n",
    "            \"window_size\": int(np.sqrt(datasets[dataset][layer].shape[1])),\n",
    "            \"missing_value\": datasets[dataset][layer]\n",
    "            .drop(\"label\", axis=1)\n",
    "            .max(axis=1)\n",
    "            .max()\n",
    "            + 1,\n",
    "        }\n",
    "        for layer in datasets[dataset]\n",
    "    }\n",
    "    for dataset in datasets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ GENERATING MAZE ----------------------------- #\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_maze(classifier: tuple, size: tuple):\n",
    "    # Unpack Arguments\n",
    "    size_x, size_y = size\n",
    "    window_size = classifier[\"window_size\"]\n",
    "    missing_value = classifier[\"missing_value\"]\n",
    "    model = classifier[\"model\"]\n",
    "    model.set_params(n_jobs=1)\n",
    "\n",
    "    # Generate Maze\n",
    "    maze = np.full((size_x + 2 * window_size, size_y + 2 * window_size), missing_value)\n",
    "    ## Randomly walk through the maze\n",
    "    for i in range(window_size, size_x + window_size):\n",
    "        for j in range(window_size, size_y + window_size):\n",
    "            input = maze[\n",
    "                i : i + window_size,\n",
    "                j : j + window_size,\n",
    "            ].flatten()\n",
    "            logits = model.predict_proba(\n",
    "                np.delete(input, (window_size**2 - 1) // 2).reshape(1, -1)\n",
    "            )[0]\n",
    "            # Select random value according to the logits\n",
    "            maze[i, j] = np.random.choice(model.classes_, p=logits / logits.sum())\n",
    "    ## Remove the window size\n",
    "    maze = maze[window_size:-window_size, window_size:-window_size]\n",
    "\n",
    "    return maze\n",
    "\n",
    "\n",
    "maze = generate_maze(classifiers[\"training_map\"][\"1\"], (100, 100))\n",
    "# Save the maze as a csv file\n",
    "np.savetxt(\"maze.csv\", maze, delimiter=\",\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
