{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération du Labyrinthe\n",
    "\n",
    "> L'objectif de ce notebook est d'entrainer un modèle à générer des labyrinthes complexes et infinis en utilisant les cartes d'exemple disponibles dans ce même répertoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération du dataset\n",
    "\n",
    "Dans un premier temps, nous cherchons à obtenir un dataset de la forme suivante :\n",
    "$$\n",
    "\\text{Voisinage} \\rightarrow \\text{Case}\n",
    "$$\n",
    "Pour ce faire, nous allons reprendre l'idée des filtres de convolution et balayer le labyrinthe avec un filtre de taille $n\\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "-> **training_map** : *1 layers*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-> **maze** : *1 layers*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------- CHARGEMENT DES DONNEES -------------------------- #\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "data = {}\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            last_character = filename[-5]\n",
    "            simple_filename = filename.split(\".\")[0]\n",
    "            if simple_filename not in data:\n",
    "                data[simple_filename] = {}\n",
    "            if last_character not in data[simple_filename]:\n",
    "                try:\n",
    "                    csv = pd.read_csv(filename).to_numpy()\n",
    "                except:\n",
    "                    continue\n",
    "                if csv.shape[0] * csv.shape[1] != 0:\n",
    "                    data[simple_filename][last_character] = csv\n",
    "\n",
    "for data_name in data:\n",
    "    display(Markdown(f\"-> **{data_name}** : *{len(data[data_name])} layers*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "-> **training_map** : *1 : 90 samples*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-> **maze** : *e : 90 samples*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------- CREATON DES DATASETS --------------------------- #\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_dataset(data, neighbourhood=5):\n",
    "    window_size = neighbourhood * 2 + 1\n",
    "\n",
    "    # Add padding of size window_size // 2 with -1\n",
    "    data = data.astype(str)\n",
    "    data = np.pad(data, window_size, constant_values=\"border\")\n",
    "    dataset = pd.DataFrame(\n",
    "        [\n",
    "            data[i : i + window_size, j : j + window_size].flatten()\n",
    "            for i in range(0, data.shape[0] - window_size + 1)\n",
    "            for j in range(0, data.shape[1] - window_size + 1)\n",
    "        ]\n",
    "    )\n",
    "    dataset[\"label\"] = dataset[dataset.columns[(window_size**2 - 1) // 2]]\n",
    "    dataset.drop(dataset.columns[(window_size**2 - 1) // 2], axis=1, inplace=True)\n",
    "    dataset = dataset[dataset[\"label\"] != \"border\"]\n",
    "    dataset = dataset.replace(\"border\", -1)\n",
    "    dataset = dataset.astype(int)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    data_name: {\n",
    "        last_character: create_dataset(data[data_name][last_character])\n",
    "        for last_character in data[data_name]\n",
    "    }\n",
    "    for data_name in data\n",
    "}\n",
    "for dataset in datasets:\n",
    "    for layer in datasets[dataset]:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"-> **{dataset}** : *{layer} : {len(datasets[dataset][layer])} samples*\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ TRAINING MODELS ----------------------------- #\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers = {\n",
    "    dataset: {\n",
    "        layer: {\n",
    "            \"model\": RandomForestClassifier(verbose=True, n_jobs=-1).fit(\n",
    "                datasets[dataset][layer].drop(\"label\", axis=1),\n",
    "                datasets[dataset][layer][\"label\"],\n",
    "            ),\n",
    "            \"window_size\": int(np.sqrt(datasets[dataset][layer].shape[1])),\n",
    "            # List of unique value in the dataframe\n",
    "            \"assets\": dict(\n",
    "                zip(\n",
    "                    *np.unique(\n",
    "                        datasets[dataset][layer].to_numpy().flatten(),\n",
    "                        return_counts=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "        for layer in datasets[dataset]\n",
    "    }\n",
    "    for dataset in datasets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ GENERATING MAZE ----------------------------- #\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_maze(classifier: tuple, size: tuple, nbr_iter=5, temperature=0.05):\n",
    "    # Unpack Arguments\n",
    "    size_x, size_y = size\n",
    "    window_size = classifier[\"window_size\"]\n",
    "    assets = classifier[\"assets\"]\n",
    "    model = classifier[\"model\"]\n",
    "    model.set_params(n_jobs=1)\n",
    "\n",
    "    # Generate Maze\n",
    "    ## Generate a matrix full of random values from the assets\n",
    "    maze = np.random.choice(\n",
    "        list(assets.keys()),\n",
    "        (size_x, size_y),\n",
    "        p=list(assets.values()) / np.sum(list(assets.values())),\n",
    "    )\n",
    "\n",
    "    # Apply the classifier on the maze\n",
    "    for _ in range(nbr_iter):\n",
    "        # Generate dataset from the maze\n",
    "        dataset = create_dataset(maze, (window_size - 1) // 2).drop(\"label\", axis=1)\n",
    "\n",
    "        # Apply the classifier on the dataset\n",
    "        logits = model.predict_proba(dataset)\n",
    "        logits = logits ** (1 / temperature)\n",
    "        predictions = np.array(\n",
    "            [\n",
    "                np.random.choice(model.classes_, p=logit / logit.sum())\n",
    "                for logit in logits\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Recreate the maze from the predictions\n",
    "        maze = np.array(predictions).reshape(size_x, size_y)\n",
    "\n",
    "    return maze\n",
    "\n",
    "\n",
    "maze = generate_maze(classifiers[\"training_map\"][\"1\"], (25, 25))\n",
    "# Save the maze as a csv file\n",
    "np.savetxt(\"maze.csv\", maze, delimiter=\",\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9d5b3657004eea8c853a01e88de3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=400, width=400)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------- PLOT MAP --------------------------------- #\n",
    "import ipycanvas\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from ipycanvas import Canvas\n",
    "\n",
    "\n",
    "def get_image_from_path(image_path):\n",
    "    # Charger l'image en utilisant Pillow\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convertir l'image en un tableau NumPy\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Créer un canvas ipycanvas de la taille de l'image\n",
    "    canvas = Canvas(width=image.width, height=image.height)\n",
    "\n",
    "    # Dessiner l'image sur le canvas en utilisant put_image_data\n",
    "    canvas.put_image_data(image_array, 0, 0)\n",
    "\n",
    "    # Retourner le canvas avec l'image dessinée dessus\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def plot_map(maze):\n",
    "    assets = {}\n",
    "    for root, dirs, files in os.walk(os.path.join(\"..\", \"frames\")):\n",
    "        # Remove files starting with a capital letter\n",
    "        files = [f for f in files if not f[0].isupper()]\n",
    "        for i, filename in enumerate(sorted(files)):\n",
    "            if i in maze:\n",
    "                assets[i] = get_image_from_path(os.path.join(root, filename))\n",
    "    # Create a black image for -1 (16x16) with alpha channel\n",
    "    assets[-1] = ipycanvas.Canvas(width=16, height=16)\n",
    "    assets[-1].fill_style = \"black\"\n",
    "    assets[-1].fill_rect(0, 0, 16, 16)\n",
    "\n",
    "    # Create the canvas\n",
    "    canvas = ipycanvas.Canvas(width=16 * maze.shape[0], height=16 * maze.shape[1])\n",
    "    canvas.fill_style = \"black\"\n",
    "    canvas.fill_rect(0, 0, 16 * maze.shape[0], 16 * maze.shape[1])\n",
    "\n",
    "    # Draw the maze\n",
    "    for i in range(maze.shape[0]):\n",
    "        for j in range(maze.shape[1]):\n",
    "            canvas.draw_image(\n",
    "                assets[maze[i, j]],\n",
    "                16 * i,\n",
    "                16 * j,\n",
    "                16,\n",
    "                16,\n",
    "            )\n",
    "    return canvas\n",
    "\n",
    "\n",
    "plot_map(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
